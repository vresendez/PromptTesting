from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def calculate_cosine_similarity(generated_text, reference_texts):
    """
    Calculates the maximum and average cosine similarity between the generated text 
    and a list of reference texts using TF-IDF.
    
    Args:
        generated_text (str): The text generated by the LLM.
        reference_texts (list): A list of reference strings (user examples).
        
    Returns:
        dict: {"max_similarity": float, "avg_similarity": float}
    """
    if not reference_texts:
        return {"max_similarity": 0.0, "avg_similarity": 0.0}
    
    # Combine all texts to fit the vectorizer
    all_texts = [generated_text] + reference_texts
    
    try:
        tfidf_vectorizer = TfidfVectorizer().fit_transform(all_texts)
        cosine_matrix = cosine_similarity(tfidf_vectorizer[0:1], tfidf_vectorizer[1:])
        
        scores = cosine_matrix[0]
        return {
            "max_similarity": float(np.max(scores)),
            "avg_similarity": float(np.mean(scores))
        }
    except ValueError:
        # Handle cases with empty vocabulary or other vectorizer issues
        return {"max_similarity": 0.0, "avg_similarity": 0.0}

def calculate_diversity_metrics(text_list):
    """
    Calculates diversity metrics for a list of texts (user stories).
    
    1. Semantic Diversity: 1 - Average Pairwise Cosine Similarity
       (Higher = More diverse meanings)
    2. Lexical Diversity (TTR): Unique Words / Total Words
    """
    if not text_list or len(text_list) < 2:
        return {"semantic_diversity": 0.0, "lexical_diversity": 0.0}
        
    # 1. Semantic Diversity
    try:
        tfidf = TfidfVectorizer().fit_transform(text_list)
        cosine_matrix = cosine_similarity(tfidf)
        
        # Exclude diagonal (self-similarity is always 1)
        # We want the average of the upper triangle
        n = cosine_matrix.shape[0]
        triu_indices = np.triu_indices(n, k=1)
        
        if len(triu_indices[0]) > 0:
            avg_sim = np.mean(cosine_matrix[triu_indices])
            semantic_diversity = 1.0 - avg_sim
        else:
            semantic_diversity = 0.0
    except ValueError:
        semantic_diversity = 0.0

    # 2. Lexical Diversity (TTR)
    try:
        all_text = " ".join(text_list).lower()
        words = all_text.split()
        if not words:
            lexical_diversity = 0.0
        else:
            lexical_diversity = len(set(words)) / len(words)
    except:
        lexical_diversity = 0.0
        
    return {
        "semantic_diversity": float(semantic_diversity),
        "lexical_diversity": float(lexical_diversity)
    }

def calculate_structure_score(text_list):
    """
    Calculates the percentage of stories that follow the valid User Story format:
    'As a <role>, I want <goal>, so that <benefit>'
    """
    if not text_list:
        return 0.0
        
    valid_count = 0
    # Basic check for key phrases
    # A robust check would verify the order, but this is a good proxy
    for story in text_list:
        lower = story.lower()
        if "as a " in lower and "want" in lower and "so that" in lower:
            valid_count += 1
            
    return (valid_count / len(text_list)) * 100.0

def calculate_readability(text_list):
    """
    Estimates Flesch-Kincaid Grade Level.
    Formula: 0.39 * (words/sentences) + 11.8 * (syllables/words) - 15.59
    """
    if not text_list:
        return 0.0
        
    all_text = " ".join(text_list)
    words = all_text.split()
    if not words: return 0.0
    
    num_words = len(words)
    num_sentences = max(1, all_text.count('.') + all_text.count('!') + all_text.count('?'))
    
    # Syllable heuristic: count vowels, roughly (very basic)
    vowels = "aeiouy"
    num_syllables = 0
    for word in words:
        word = word.lower().strip(".:;?!")
        if not word: continue
        count = 0
        if word[0] in vowels:
            count += 1
        for index in range(1, len(word)):
            if word[index] in vowels and word[index - 1] not in vowels:
                count += 1
        if word.endswith("e"):
            count -= 1
        if count == 0:
            count = 1
        num_syllables += count
        
    grade = 0.39 * (num_words / num_sentences) + 11.8 * (num_syllables / num_words) - 15.59
    return max(0.0, grade)

def calculate_concept_coverage(generated_text, reference_texts):
    """
    Calculates the percentage of unique 'concepts' (words > 4 chars) from the 
    Human Reference that are present in the Generated Text.
    Proxy for 'Requirement Coverage'.
    """
    if not reference_texts:
        return 0.0
        
    gen_words = set([w.lower().strip(".,;?!") for w in generated_text.split() if len(w) > 4])
    
    ref_blob = " ".join(reference_texts)
    ref_words = set([w.lower().strip(".,;?!") for w in ref_blob.split() if len(w) > 4])
    
    if not ref_words:
        return 0.0
        
    overlap = gen_words.intersection(ref_words)
    return len(overlap) / len(ref_words)
